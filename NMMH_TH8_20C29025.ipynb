{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j411dkkAuSvt"
   },
   "source": [
    "**Lab-08: Backpropagation**\n",
    "\n",
    "Trong bài thực hành này chúng ta sẽ thử cài đặt Backpropagation \n",
    "\n",
    "Ta muốn dựa vào 2 chiều của lá, phân biệt giữa loại lá 1 và loại lá 2. Cụ thể, với $x = (x_1,x_2, 1)$ là input, ta muốn đoán một phân phối\n",
    "    $$ P_\\\\theta(c|x),c = 0, 1 $$\n",
    "với $\\\\theta$ là các tham số\n",
    "Ta mô hình $P_\\theta$ là một neural network có 2 lớp ẩn, mỗi lớp 5 neurons, tức là\\n\",\n",
    "    $$ P_\\\\theta(c|x) = \\\\text{softmax}(\\\\max(0, \\\\max(0, x \\\\cdot W_1 + b_1) \\\\cdot W_2 + b_2) \\\\cdot W_3 + b_3 )$$\n",
    "\n",
    "với $x$ là vector dòng $[[x_1, x_2]]$ kích thước $ 1\\times 2$, $W_1, W_2, W_3$ là các ma trận có kích thước $2 \\times 5, 5 \\times 5, 5 \\times 3$, và $b_1, b_2, b_3$ là các ma trận kích thước $1 \\times 5, 1 \\times 5, 1 \\times 3$.\n",
    "\n",
    "Khi đó $P(c|x)$ là một vector dòng độ dài 3, xem như $P(c|x)= (P_1(c|x), P_2(c|x), P_3(c|x)) = (P(c=0|x), P(c=1|x), P(c=2|x))$\n",
    "Bộ các ma trận $\\\\theta = (W_1, W_2, W_3, b_1, b_2, b_3)$ chính là tham số cần tìm của model. Giờ cần tìm $\\\\theta$ sao cho \n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_{x,y} - y_0 \\log P_\\theta(0|x) -  y_1 \\log P_\\theta(1|x) - y_2 \\log P_\\theta(2|x) $$\n",
    "\n",
    "đạt giá trị nhỏ nhất với $y = (y_0, y_1, y_2)$ là one-hot vector biểu thị loại lá tương ứng với $x$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hRjcNaFBigAx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zS_ADnTigA3"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DzecGvNIigA4"
   },
   "outputs": [],
   "source": [
    "def one_hot_vector(y):\n",
    "    out = np.zeros((y.shape[0], max(y)+1))\n",
    "    for i in range(y.shape[0]):\n",
    "        out[i, y[i]] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NpL3fpASigA5"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"https://raw.githubusercontent.com/huynhthanh98/ML/master/lab-08/bt_train.csv\")\n",
    "valid = pd.read_csv(\"https://raw.githubusercontent.com/huynhthanh98/ML/master/lab-08/bt_valid.csv\")\n",
    "\n",
    "x1_train = train[\"x1\"].values\n",
    "x2_train = train[\"x2\"].values\n",
    "y_train = train[\"label\"].values\n",
    "\n",
    "x1_valid = valid['x1'].values\n",
    "x2_valid = valid['x2'].values\n",
    "y_valid = valid['label'].values\n",
    "\n",
    "# normalize\n",
    "x1_mean = np.mean(x1_train)\n",
    "x1_std = np.std(x1_train)\n",
    "x2_mean = np.mean(x2_train)\n",
    "x2_std = np.std(x2_train)\n",
    "\n",
    "x1_train = (x1_train - x1_mean)/ x1_std\n",
    "x2_train = (x2_train - x2_mean)/ x2_std\n",
    "\n",
    "x1_valid = (x1_valid - x1_mean)/ x1_std\n",
    "x2_valid = (x2_valid - x2_mean)/ x2_std\n",
    "\n",
    "\n",
    "\n",
    "X_train = np.concatenate([x1_train.reshape(-1,1), x2_train.reshape(-1,1)], axis=1)\n",
    "y_train = one_hot_vector(y_train)\n",
    "\n",
    "X_valid = np.concatenate([x1_valid.reshape(-1,1), x2_valid.reshape(-1,1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4_CidQJtigA7"
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "W1 = np.random.randn(2,5)\n",
    "W2 = np.random.randn(5,5)\n",
    "W3 = np.random.randn(5,3)\n",
    "\n",
    "b1 = np.random.randn(1,5)\n",
    "b2 = np.random.randn(1,5)\n",
    "b3 = np.random.randn(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Kw8mSDk6igA9"
   },
   "outputs": [],
   "source": [
    "def relu(h):\n",
    "    return np.array([max(0,i) for i in h.reshape(-1)]).reshape(h.shape)\n",
    "\n",
    "def softmax(z):\n",
    "      return np.exp(z)/ np.sum(np.exp(z), axis=1).reshape(-1,1)\n",
    "\n",
    "def CrossEntropy(o,y):\n",
    "      return - np.sum(np.log(o)*y)\n",
    "\n",
    "ln = 0.001\n",
    "N = y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vjLI8EDoigA-"
   },
   "outputs": [],
   "source": [
    "for epochs in range(100000):\n",
    "    # foward\n",
    "    z1 = np.dot(X_train, W1) + b1\n",
    "    o1 = relu(z1)\n",
    "\n",
    "    z2 = np.dot(o1, W2) + b2\n",
    "    o2 = relu(z2)\n",
    "\n",
    "    z3 = np.dot(o2, W3) + b3\n",
    "    o3 = softmax(z3)\n",
    "\n",
    "    # backpropagation\n",
    "    dL_dz3 = 1/len(X_train)*(o3 - y_train) \n",
    "    dL_dW3 = np.dot(o2.T, dL_dz3)    \n",
    "    dL_db3 = np.sum(dL_dz3, axis = 0)\n",
    "\n",
    "\n",
    "    dL_do2 = np.dot(dL_dz3, W3.T)\n",
    "    dL_dz2 = dL_do2.copy()\n",
    "    dL_dz2[z2 < 0] = 0\n",
    "    dL_dW2 = np.dot(o1.T,dL_dz2)\n",
    "    dL_db2 = np.sum(dL_dz2, axis = 0)\n",
    "\n",
    "    dL_do1 = np.dot(dL_dz2, W2.T)\n",
    "    dL_dz1 = dL_do1.copy()\n",
    "    dL_dz1[z1 < 0] = 0\n",
    "    dL_dW1 = np.dot(X_train.T, dL_dz1)\n",
    "    dL_db1 = np.sum(dL_dz1, axis = 0)\n",
    "\n",
    "    W3 -= ln* dL_dW3\n",
    "    b3 -= ln* dL_db3\n",
    "    W2 -= ln* dL_dW2\n",
    "    b2 -= ln* dL_db2\n",
    "    W1 -= ln* dL_dW1\n",
    "    b1 -= ln* dL_db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DmSjKNZpigBA"
   },
   "outputs": [],
   "source": [
    "z1_valid = np.dot(X_valid, W1) + b1\n",
    "o1_valid = relu(z1_valid)\n",
    "\n",
    "z2_valid = np.dot(o1_valid, W2) + b2\n",
    "o2_valid = relu(z2_valid)\n",
    "\n",
    "z3_valid = np.dot(o2_valid, W3) + b3\n",
    "o3_valid = softmax(z3_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHthk1kuigBB",
    "outputId": "d3b0f643-fab9-472b-b78e-fbe99b1f3938"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6366666666666667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.argmax(o3_valid, axis = 1) == y_valid)/ y_valid.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSZlBMhnewyH"
   },
   "source": [
    "#Bài Tập\n",
    "1. Từ code demo hãy cài đặt thêm một module để chọn ra được bộ weights sao cho accuracy trên tập validation là tốt nhất.\n",
    "2. Từ bộ dữ liệu bên dưới hãy cài đặt backpropagation cho bài toán phân biệt ung thư vú. Hãy tự chọn số layers và số nodes mà mình cho là thích hợp, cũng như là nêu ra số layers và số nodes của mỗi layer mà mình đã chọn. Tính accuracy trên tập training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJ6ss-qM6nrK"
   },
   "source": [
    "Câu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "W1 = np.random.randn(2,5)\n",
    "W2 = np.random.randn(5,5)\n",
    "W3 = np.random.randn(5,3)\n",
    "\n",
    "b1 = np.random.randn(1,5)\n",
    "b2 = np.random.randn(1,5)\n",
    "b3 = np.random.randn(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4x7T_rnj6i0B",
    "outputId": "83ccbb1a-61bd-4ef9-89b2-11d8108a16ad"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_valid_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-2c59a9713fd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mvalid_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo3_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mvalid_accuracy\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_valid_accuracy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mmax_valid_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mepoch_opti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_valid_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(100000):\n",
    "    #forward\n",
    "    z1 = np.dot(X_train, W1) + b1\n",
    "    o1 = relu(z1)\n",
    "    \n",
    "    z2 = np.dot(o1, W2) + b2\n",
    "    o2 = relu(z2)\n",
    "    \n",
    "    z3 = np.dot(o2, W3) + b3\n",
    "    o3 = softmax(z3)\n",
    "    \n",
    "    #backpropagation\n",
    "    dL_dz3 = 1/len(X_train)*(o3 - y_train)\n",
    "    dL_dW3 = np.dot(o2.T, dL_dz3)\n",
    "    dL_db3 = np.sum(dL_dz3, axis = 0)\n",
    "    \n",
    "    \n",
    "    dL_do2 = np.dot(dL_dz3, W3.T)\n",
    "    dL_dz2 = dL_do2.copy()\n",
    "    dL_dz2[z2 < 0] = 0\n",
    "    dL_db2 = np.sum(dL_dz2, axis = 0)\n",
    "    \n",
    "    dL_do1 = np.dot(dL_dz2, W2.T)\n",
    "    dL_dz1 = dL_do1.copy()\n",
    "    dL_dz1[z1 < 0] = 0\n",
    "    dL_dW1 = np.dot(X_train.T, dL_dz1)\n",
    "    dL_db1 = np.sum(dL_dz1, axis = 0)\n",
    "\n",
    "    W3 -= ln* dL_dW3\n",
    "    b3 -= ln* dL_db3\n",
    "    W2 -= ln* dL_dW2\n",
    "    b2 -= ln* dL_db2\n",
    "    W1 -= ln* dL_dW1\n",
    "    b1 -= ln* dL_db1\n",
    "\n",
    "    z1_valid = np.dot(X_valid, W1) + b1\n",
    "    o1_valid = relu(z1_valid)\n",
    "\n",
    "    z2_valid = np.dot(o1_valid, W2) + b2\n",
    "    o2_valid = relu(z2_valid)\n",
    "\n",
    "    z3_valid = np.dot(o2_valid, W3) + b3\n",
    "    o3_valid = softmax(z3_valid)\n",
    "\n",
    "    valid_accuracy = np.sum(np.argmax(o3_valid, axis = 1) == y_valid)/ y_valid.shape[0]\n",
    "\n",
    "    if valid_accuracy > max_valid_accuracy:\n",
    "        max_valid_accuracy = valid_accuracy\n",
    "        epoch_opti = epoch\n",
    "\n",
    "        W3_opti = W3\n",
    "        b3_opti = b3\n",
    "        W2_opti  = W2\n",
    "        b2_opti = b2\n",
    "        W1_opti  = W1\n",
    "        b1_opti = b1\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Accuracy in validation set at epoch {epoch} is {valid_accuracy:.4f}\")\n",
    "\n",
    "print(f\"Highest Accuracy in validation set obtained at epoch {epoch_opti} is {max_valid_accuracy:.4f}\")\n",
    "print(f\"Optimal weight are storaged in Wx_opti and bx_opti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta thấy Accuracy cao nhất đạt được là ở epoch cuối cùng trong quá trình train. \n",
    "Do đó có thể thấy là mô hình có khả năng cao là chưa bị overfitting. Có thể tăng số epoch train để mô hình hội tụ tốt hơn\n",
    "\n",
    "Bộ parameter 𝑡ℎ𝑒𝑡𝑎=(𝑊1,𝑊2,𝑊3,𝑏1,𝑏2,𝑏3) được lưu lại khi mỗi epoch train trên tập training set mà có Accuracy tăng so với Accuracy cao nhất đã đạt được trước đó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAUX9Vdk6rh4"
   },
   "source": [
    "Câu 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NscLiyVYuPx"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data  \n",
    "y = breast_cancer.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split( X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_mean=np.mean(X_train)\n",
    "X_std=np.std(X_train)\n",
    "\n",
    "X_valid=(X_valid-X_mean)/X_std\n",
    "X_train=(X_train-X_mean)/X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Crm2UvQi8Spi"
   },
   "outputs": [],
   "source": [
    "#Sử dụng one hot vector cho y_train để tính Loss với hàm softmax ở layer cuối\n",
    "y_train_1hot = one_hot_vector(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JC-cEdyg6wq4"
   },
   "outputs": [],
   "source": [
    "print(list(breast_cancer.feature_names))\n",
    "print(f'Số lượng features input: {len(list(breast_cancer.feature_names))}')\n",
    "\n",
    "print(f'Số lượng classes output: {len(list(np.unique(y)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bộ dữ liệu về ung thư vú có 30 features ứng với 30 nodes input, 2 nodes output\n",
    "\n",
    "Ta thiết kế mạng Neuron với 3 hidden layer với số nodes lần  lượt là <br>\n",
    "Lớp 1: 15 node, Lớp 2: 15 node, Lớp 3: 5 node\n",
    "\n",
    "\n",
    "Kích thước các ma trận W1, W2, W3, W4 tương ứng là: 30x15, 15x15, 15x5, 5x2 <br>\n",
    "Kích thước các ma trận W1, W2, W3, W4 là: 1x15, 1x15, 1x5, 1x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FH7G3lx33R5A"
   },
   "outputs": [],
   "source": [
    "# initialize\n",
    "W1 = np.random.randn(30,15)\n",
    "W2 = np.random.randn(15,15)\n",
    "W3 = np.random.randn(15,5)\n",
    "W4 = np.random.randn(5,2)\n",
    "\n",
    "b1 = np.random.randn(1,15)\n",
    "b2 = np.random.randn(1,15)\n",
    "b3 = np.random.randn(1,5)\n",
    "b4 = np.random.randn(1,2)\n",
    "\n",
    "ln = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvlgt9kn3GVt"
   },
   "outputs": [],
   "source": [
    "max_valid_accuracy = 0\n",
    "\n",
    "for epoch in range(100000):\n",
    "    #foward\n",
    "    z1 = np.dot(X_train, W1) + b1\n",
    "    o1 = relu(z1)\n",
    "\n",
    "    z2 = np.dot(o1, W2) + b2\n",
    "    o2 = relu(z2)\n",
    "\n",
    "    z3 = np.dot(o2, W3) + b3\n",
    "    o3 = relu(z3)\n",
    "\n",
    "    z4 = np.dot(o3, W4) + b4\n",
    "    o4 = softmax(z4)\n",
    "\n",
    "    #backpropagation\n",
    "    dL_dz4 = 1/len(X_train)*(o4 - y_train_1hot) \n",
    "    dL_dW4 = np.dot(o3.T, dL_dz4)    \n",
    "    dL_db4 = np.sum(dL_dz4, axis = 0)\n",
    "\n",
    "    dL_do3 = np.dot(dL_dz4, W4.T)\n",
    "    dL_dz3 = dL_do3.copy()\n",
    "    dL_dz3[z3 < 0] = 0\n",
    "    dL_dW3 = np.dot(o2.T,dL_dz3)\n",
    "    dL_db3 = np.sum(dL_dz3, axis = 0)\n",
    "\n",
    "    dL_do2 = np.dot(dL_dz3, W3.T)\n",
    "    dL_dz2 = dL_do2.copy()\n",
    "    dL_dz2[z2 < 0] = 0\n",
    "    dL_dW2 = np.dot(o1.T,dL_dz2)\n",
    "    dL_db2 = np.sum(dL_dz2, axis = 0)\n",
    "\n",
    "    dL_do1 = np.dot(dL_dz2, W2.T)\n",
    "    dL_dz1 = dL_do1.copy()\n",
    "    dL_dz1[z1 < 0] = 0\n",
    "    dL_dW1 = np.dot(X_train.T, dL_dz1)\n",
    "    dL_db1 = np.sum(dL_dz1, axis = 0)\n",
    "\n",
    "    W4 -= ln* dL_dW4\n",
    "    b4 -= ln* dL_db4\n",
    "    W3 -= ln* dL_dW3\n",
    "    b3 -= ln* dL_db3\n",
    "    W2 -= ln* dL_dW2\n",
    "    b2 -= ln* dL_db2\n",
    "    W1 -= ln* dL_dW1\n",
    "    b1 -= ln* dL_db1\n",
    "\n",
    "    z1_valid = np.dot(X_valid, W1) + b1\n",
    "    o1_valid = relu(z1_valid)\n",
    "\n",
    "    z2_valid = np.dot(o1_valid, W2) + b2\n",
    "    o2_valid = relu(z2_valid)\n",
    "\n",
    "    z3_valid = np.dot(o2_valid, W3) + b3\n",
    "    o3_valid = softmax(z3_valid)\n",
    "\n",
    "    z4_valid = np.dot(o3_valid, W4) + b4\n",
    "    o4_valid = softmax(z4_valid)\n",
    "\n",
    "    valid_accuracy = np.sum(np.argmax(o4_valid, axis = 1) == y_valid)/ y_valid.shape[0]\n",
    "\n",
    "    if valid_accuracy > max_valid_accuracy:\n",
    "        max_valid_accuracy = valid_accuracy\n",
    "        epoch_opti = epoch\n",
    "\n",
    "        W4_opti = W4\n",
    "        b4_opti = b4\n",
    "        W3_opti = W3\n",
    "        b3_opti = b3\n",
    "        W2_opti  = W2\n",
    "        b2_opti = b2\n",
    "        W1_opti  = W1\n",
    "        b1_opti = b1\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Accuracy in validation set at epoch {epoch} is {valid_accuracy:.4f}\")\n",
    "\n",
    "print(f\"Highest Accuracy in validation set obtained at epoch {epoch_opti} is {max_valid_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOulDx8pDnnC"
   },
   "outputs": [],
   "source": [
    "#Sử dụng bộ Parameters tốt được đã thu được sau khi train 10000 epochs\n",
    "#Tính toán quá trình forward với input là bộ data train\n",
    "z1_train = np.dot(X_train, W1_opti) + b1_opti\n",
    "o1_train = relu(z1_train)\n",
    "\n",
    "z2_train = np.dot(o1_train, W2_opti) + b2_opti\n",
    "o2_train = relu(z2_train)\n",
    "\n",
    "z3_train = np.dot(o2_train, W3_opti) + b3_opti\n",
    "o3_train = softmax(z3_train)\n",
    "\n",
    "z4_train = np.dot(o3_train, W4_opti) + b4_opti\n",
    "o4_train = softmax(z4_train)\n",
    "\n",
    "#So sánh Label tập train và Kết quả quá trình forward trên mạng Neuron \n",
    "np.sum(np.argmax(o4_train, axis = 1) == y_train)/ y_train.shape[0]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab_08.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
